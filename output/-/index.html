<!DOCTYPE html>
<!--suppress ALL -->
<html lang="ja">
<head>
    <meta name="generator" content="Quarkdown">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>卒業論文見本</title>
    <script src="./script/quarkdown.js"></script>
    <script>const capabilities = window.quarkdownCapabilities</script>
    <script>window.PagedConfig = {auto: false};</script>
    <script src="https://unpkg.com/pagedjs@0.4.3/dist/paged.polyfill.js"></script>
    <link href='https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css' rel='stylesheet'>
    <link rel="stylesheet" href="./theme/theme.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js"></script>
    <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css"/>
    <script>capabilities.code = true;</script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>
    <script>
        capabilities.math = true;

        window.texMacros = {
        }
    </script>

    <style>

        body {
        }

        .page-break {
            break-before: always;
        }

        .quarkdown-plain .page-break {
            break-before: avoid;
            break-after: avoid;
        }

        body.quarkdown-plain.quarkdown-plain {
            margin: 1.5cm 1.5cm 1.5cm 1.5cm;
        }

        body.quarkdown-slides.quarkdown-slides .reveal {
            width: 210.0mm;
            height: 297.0mm;
        }

        @page {
            size: 210.0mm 297.0mm;
            margin: 1.5cm 1.5cm 1.5cm 1.5cm;
        }

        p {
        }
    </style>
    <script>
        const doc = new PagedDocument();
        prepare(doc);
    </script>
</head>
<body class="quarkdown quarkdown-paged">
<div class="page-margin-content page-margin-bottom-center" data-on-left-page="bottom-center" data-on-right-page="bottom-center"><span></span></div><style data-hidden="">.quarkdown h1 {font-size: 1.5rem !important;}
.quarkdown h2 {font-size: 1.3rem !important;}
.quarkdown h3 {font-size: 1.15rem !important;}
.quarkdown h4 {font-size: 1.05rem !important;}
/* Cross-reference: show Japanese style for section references */
/* Top-level sections (no dot): 第1章 */
.quarkdown .cross-reference[data-localized-kind="セクション"][data-location]:not([data-location*="."])::before {
    content: "第" attr(data-location) "章" !important;
}
/* Sub-sections (has dot): 第2.1節 */
.quarkdown .cross-reference[data-localized-kind="セクション"][data-location*="."]::before {
    content: "第" attr(data-location) "節" !important;
}
.textcenter {
    text-align: center;
}</style><p>
</p><div style="justify-content: flex-start; align-items: center;" class="stack stack-column"><span class="size-normal">2026年度 卒業論文</span><div style="height: 5.0mm;"></div><span class="size-large textcenter" style="font-weight: bold;">Notionデータを対象とした<br />                    ベクトル検索システム<br />                    「NotionFinder」の開発</span><span></span></div><div style="justify-content: flex-start; align-items: flex-end;" class="stack stack-column"><span>龍谷大学先端理工学部 数理・情報科学課程</span><span>Y220010 田代 圭梧</span><span>指導教員 佐野 彰</span></div><div style="justify-content: flex-start; align-items: center;" class="stack stack-column"><span class="size-normal" style="font-weight: bold;">概要</span></div><p>まだ</p><p>書かない</p><div class="page-break" data-hidden=""></div><div style="height: 5.0cm;"></div><div style="justify-content: flex-start; align-items: center;" class="stack stack-column"><span>2026年度 卒業論文</span><div style="height: 10.0mm;"></div><span class="size-large textcenter" style="font-weight: bold;">Notionデータを対象とした<br />                    ベクトル検索システム<br />                    「NotionFinder」の開発</span><p><span class="size-normal"></span></p></div><div style="height: 10.0cm;"></div><div style="justify-content: flex-start; align-items: center; gap: 3.0mm;" class="stack stack-column"><span>龍谷大学先端理工学部 数理・情報科学課程</span><span>Y220010 田代 圭梧</span><span>指導教員 佐野 彰</span></div><p><div class="page-break" data-hidden=""></div><h1 id="table-of-contents">目次</h1><nav data-role="table-of-contents"><ol><li data-location="1"><a href="#sec:intro">はじめに</a><ol><li data-location="1.1"><a href="#研究背景">研究背景</a></li><li data-location="1.2"><a href="#本研究の目的">本研究の目的</a></li></ol></li><li data-location="2"><a href="#sec:method">NotionFinderのシステム概要</a><ol><li data-location="2.1"><a href="#前処理パイプライン">前処理パイプライン</a></li><li data-location="2.2"><a href="#webアプリケーション実行環境">Webアプリケーション実行環境</a></li></ol></li><li data-location="3"><a href="#sec:techstack">技術スタック</a><ol><li data-location="3.1"><a href="#アプリケーション基盤">アプリケーション基盤</a></li><li data-location="3.2"><a href="#検索ベクトル処理エンジン">検索・ベクトル処理エンジン</a></li><li data-location="3.3"><a href="#notionデータ取得">Notionデータ取得</a></li></ol></li><li data-location="4"><a href="#sec:systemdetail">システム詳細</a><ol><li data-location="4.1"><a href="#前処理パイプライン">前処理パイプライン</a></li><li data-location="4.2"><a href="#webアプリケーション実行環境">Webアプリケーション実行環境</a></li></ol></li><li data-location="5"><a href="#動作検証">動作検証</a></li><li data-location="6"><a href="#終わりに">終わりに</a></li></ol></nav> </p><div class="page-break" data-hidden=""></div><h1 id="sec:intro" data-location="1">はじめに</h1><div class="page-number-reset" data-start="1" data-hidden=""></div><div class="page-margin-content page-margin-bottom-center" data-on-left-page="bottom-center" data-on-right-page="bottom-center"><span class="current-page-number">-</span></div><h2 id="研究背景" data-location="1.1">研究背景</h2><h3 id="notionの普及と情報量の増大">Notionの普及と情報量の増大</h3><p>近年、個人のタスク管理から企業のナレッジ共有に至るまで、クラウドベースのコラボレーションツールの利用が急速に拡大している。
中でも Notion Labs Inc. が提供する「Notion」は、ドキュメント作成、データベース管理、タスク管理などを一元化できる「オールインワン・ワークスペース」として、世界中で多くのユーザーを獲得している [???]。
Notion の柔軟なデータベース機能は、ユーザーによる自由度の高い情報蓄積を可能にする一方で、長期的な運用に伴いワークスペース内のデータ量は膨大かつ複雑化する傾向にある。</p><h3 id="既存検索機能の課題">既存検索機能の課題</h3><p>蓄積された膨大なデータから必要な情報にアクセスするためには、効率的な検索機能が不可欠である。
しかし、Notion に標準搭載されている検索機能は、基本的にキーワードマッチング（字句検索）に基づいている。
この手法は、クエリとドキュメント内の単語が完全に一致する場合においては有効であるが、同義語や表記揺れ、あるいは文脈的な意味の類似性を考慮することはできないという問題を抱えている。
そのため、ユーザーは正確なキーワードを記憶していなければ目的の情報に辿り着けず、情報の再利用性が著しく低下するという課題がある。</p><h3 id="notion-ai-の制約">Notion AI の制約</h3><p>こうした課題に対し、Notion は大規模言語モデル（LLM）を活用した「Notion AI」を導入し、自然言語による質問応答機能を提供している [???]。
しかし、Notion AI を制限なく利用するには追加のコストが発生するため、全てのユーザーにとって導入が容易なわけではない。
さらに重要な課題として、Web共有（外部公開）における制約が挙げられる。
Notion AI は基本的にログインユーザーのワークスペース内での利用を前提としており、Notionで構築したナレッジベースをWebサイトとして外部公開し、そこで不特定多数の訪問者にAI検索を提供するといった柔軟な運用は困難である。</p><h2 id="本研究の目的" data-location="1.2">本研究の目的</h2><p>本研究では、Notion 内に蓄積されたデータを対象とし、文脈や意味に基づいた検索（セマンティック検索）を可能にするWebプラットフォーム「NotionFinder」の設計および開発を行う。
具体的には、Notionから取得したデータをベクトル化し、ユーザーの検索クエリとの意味的な類似度を計算することで、キーワードが完全に一致しなくとも関連性の高いドキュメントを抽出するシステムを構築する。</p><p>本研究の主な目的は以下の2点である。</p><ol><li><strong>セマンティック検索の実現</strong>：キーワード検索では拾いきれない、文脈や意図を汲んだ検索体験を提供し、情報の再利用性を向上させること。</li><li><strong>Web共有可能な検索基盤の構築</strong>：Notion 本体の認証や有料プランの制約に依存せず、外部公開されたWebアプリケーション上で、安価かつ高速に高度な検索機能を利用可能にすること。</li></ol><p>次章以降では、関連技術の解説、NotionFinder のシステム設計、および実装と評価について述べる。</p><div class="page-break" data-hidden=""></div><h1 id="sec:method" data-location="2">NotionFinderのシステム概要</h1><figure id="figure-2.1"><img src="media/systemOverview@-629072555.png" alt="Quarkdown" title="NotionFinderのシステム概要" style="width: 50.0%;" /><figcaption class="caption-bottom" data-location="2.1" data-localized-kind="図">NotionFinderのシステム概要</figcaption></figure><p><span class="cross-reference" data-location="2.1" data-localized-kind="図"></span> は NotionFinder のシステム概要である。
本システムは、Notionからデータを抽出し検索可能な形式に変換する「前処理パイプライン（ローカル環境）」と、構築されたインデックスを用いてユーザーに検索機能を提供する「Webアプリケーション実行環境」の2層から構成される。</p><h2 id="前処理パイプライン" data-location="2.1">前処理パイプライン</h2><p>検索の基盤となるデータの準備は、以下のフローで行われる。</p><p><strong>Notionデータ取得</strong></p><ul><li>Notionが提供する外部連携用インターフェースを利用し、Notionデータを取得する。</li></ul><p><strong>ベクトル化およびインデックス作成処理</strong></p><ul><li>取得したテキストデータを、埋め込みモデルを用いて高次元のベクトルデータへ変換する。変換されたベクトル群は、高速な類似度検索を可能にするためのインデックスファイルとして構造化・保存される。</li></ul><p><strong>デプロイ</strong></p><ul><li>ローカル環境で構築されたインデックスファイルを、Webアプリケーション実行環境へ配置する。</li></ul><h2 id="webアプリケーション実行環境" data-location="2.2">Webアプリケーション実行環境</h2><p>ユーザーによる検索リクエストは、Webアプリケーション実行環境上で以下の手順で処理される。</p><p><strong>検索クエリのベクトル化</strong></p><ul><li>ユーザーが入力した自然言語による検索クエリを、インデックス構築時と同一のモデルを用いてサーバーサイドでベクトル化する。</li></ul><p><strong>インデックスのロード</strong></p><ul><li>配置済みのインデックスデータをサーバーサイドのメモリ上にロードし、検索可能な状態に展開する。</li></ul><p><strong>類似度比較および抽出</strong></p><ul><li>クエリベクトルとインデックス内の各ベクトルとの間で類似度計算を行い、意味的な関連性が高い上位のドキュメントを抽出する。</li></ul><p><strong>検索結果の表示</strong></p><ul><li>抽出された結果に基づき、該当するNotionページのデータ、およびNotionのオリジナルのページへのリンクをユーザーのブラウザ上に表示する。</li></ul><div class="page-break" data-hidden=""></div><h1 id="sec:techstack" data-location="3">技術スタック</h1><h2 id="アプリケーション基盤" data-location="3.1">アプリケーション基盤</h2><p><strong>TypeScript</strong></p><ul><li>本システムの主要な開発言語として採用した。静的型付けにより、開発段階でのエラー検知やコードの保守性を向上させている。特に、Notion API から取得する複雑な入れ子構造を持つデータを扱う際、厳密な型定義を行うことでデータ処理の安全性を担保している。</li></ul><p><strong>Node.js</strong>
サーバーサイド JavaScript の実行環境として採用した。Web アプリケーションのサーバーサイド処理だけでなく、ローカル環境における前処理パイプラインの実行基盤としても使用している。</p><p><strong>Next.js (App Router)</strong></p><ul><li>Webアプリケーションのフレームワークには、React ベースの Next.js を採用した。サーバーサイドでのデータ処理とクライアントサイドのインタラクションを統合的に扱えるため、検索クエリの処理や結果表示を効率的に実装可能である。</li></ul><p><strong>GitHub</strong></p><ul><li>ソースコードのバージョン管理およびホスティングサービスとして採用した。AWS Amplify と連携し、リモートリポジトリへの変更（プッシュ）をトリガーとしてビルドおよびデプロイプロセスを自動的に開始する、CD パイプラインの起点としての役割を担っている。</li></ul><p><strong>AWS Amplify</strong></p><ul><li>アプリケーションのホスティングおよび CD（継続的デリバリー）環境として採用した。GitHub 上のリポジトリと同期し、ソースコードの更新に合わせて自動的に本番環境への反映を行うパイプラインを構築している。</li></ul><h2 id="検索ベクトル処理エンジン" data-location="3.2">検索・ベクトル処理エンジン</h2><p><strong>Xenova (transformers.js)</strong></p><ul><li>自然言語のベクトル化（Embedding）を行うためのライブラリとして採用した。通常、Transformer モデルの推論には Python 環境が必要となるが、本ライブラリを用いることで、Next.js が動作する Node.js 環境上で直接、かつ高速にベクトル化処理を実行可能とした。</li></ul><p><strong>HNSWLib (hnswlib-node)</strong></p><ul><li>HNSW (Hierarchical Navigable Small World) アルゴリズムを実装したライブラリである。本ライブラリは C++ で記述されたコアエンジンの Node.js 向けネイティブバインディングとなっており、JavaScript の実行オーバーヘッドを回避した高速なベクトル演算が可能である。これにより、数千件規模のデータセットに対しても計算コストを抑えた効率的な検索を実現している。</li></ul><h2 id="notionデータ取得" data-location="3.3">Notionデータ取得</h2><p><strong>Notion API</strong> 検索対象となるNotionデータを取得するために、Notion 公式のインターフェースを使用した。</p><div class="page-break" data-hidden=""></div><h1 id="sec:systemdetail" data-location="4">システム詳細</h1><h2 id="前処理パイプライン" data-location="4.1">前処理パイプライン</h2><h3 id="前処理パイプライン全体像">前処理パイプライン全体像</h3><figure id="figure-4.1"><img src="media/preprocessingPipeline@1697499011.png" alt="Quarkdown" title="前処理パイプラインの全体処理フロー" style="width: 100.0%;" /><figcaption class="caption-bottom" data-location="4.1" data-localized-kind="図">前処理パイプラインの全体処理フロー</figcaption></figure><p><span class="cross-reference" data-location="4.1" data-localized-kind="図"></span> に、本システムにおける前処理パイプラインの全体フローを示す。
このパイプラインは、Notion 上の非構造化データをセマンティック検索可能な形式へ変換し、Web アプリケーションとして公開するまでの<strong>データの取得から加工、保存に至る一連の処理</strong>を担うものである。
処理は主にローカル開発環境上で実行され、以下の4つの主要なフェーズで構成される。</p><ol><li><p><strong>Notion APIを用いたNotionデータ取得</strong>
Notion API を介して、指定されたデータベース内の全ページおよびブロック情報を再帰的に取得する。同時に、画像のリンク切れを防ぐため、メディアファイルのダウンロードとパスの書き換えを行い、永続化された静的アセットとして保存する。</p></li><li><p><strong>埋め込みベクトルの生成</strong>
取得したテキストデータを、ローカル環境上の軽量機械学習モデル（Xenova/multilingual-e5-small）に入力し、各ブロックの意味を表す高次元ベクトル（384次元）へ変換する。</p></li><li><p><strong>インデックスの構築戦略と最適化</strong>
生成されたベクトル群を、HNSW アルゴリズムを用いて高速検索可能なグラフ構造（インデックス）として構築し、メタデータと共に静的ファイル（JSONおよびバイナリ形式）として出力する。</p></li><li><p><strong>Amplifyへの更新データのデプロイ</strong>
生成されたインデックスファイルおよび静的アセットを Git リポジトリへコミット・プッシュする。これをトリガーとして AWS Amplify の CI/CD パイプラインが起動し、最新の検索データが本番環境へ自動的に反映される。</p></li></ol><p>次節より、各フェーズにおける技術的な詳細と実装上の工夫について述べる。</p><h3 id="notion-apiを用いたnotionデータ取得">Notion APIを用いたNotionデータ取得</h3><h4 id="notion-sdk-の活用と型安全性">Notion SDK の活用と型安全性</h4><p>Notion API との通信実装には、公式 JavaScript クライアントライブラリである <span class="codespan-content"><code>@notionhq/client</code></span> (v2.3.0) を採用した。
本ライブラリは、<span class="codespan-content"><code>search</code></span> や <span class="codespan-content"><code>blocks.children.list</code></span> といった各エンドポイントへのアクセスをメソッドとして抽象化しているほか、API レスポンスに対する厳密な TypeScript 型定義（<span class="codespan-content"><code>BlockObjectResponse</code></span>, <span class="codespan-content"><code>NotionPageData</code></span> 等）を提供している。
これにより、開発時には静的な型チェックが可能となり、複雑なプロパティを持つ Notion オブジェクトを扱う際の実行時エラーを未然に防いでいる。</p><h4 id="階層構造データの再帰的取得">階層構造データの再帰的取得</h4><p>Notion のページデータは「ブロック」と呼ばれる単位で構成されており、各ブロックはさらに子ブロックを持つことができる（例：リストの中にさらにリストがある、トグルの中に画像がある等）。このため、単一の API リクエストではページ内の全情報を取得できない場合がある。</p><p>本システムでは、この入れ子構造に対応するため、深さ優先探索（Depth-First Search）に基づいた再帰的なデータ取得アルゴリズムを実装した。
具体的には、ブロック情報の取得時に <span class="codespan-content"><code>has_children</code></span> プロパティを確認し、子要素が存在する場合は再帰的に <span class="codespan-content"><code>blocks.children.list</code></span> エンドポイントを呼び出す。これにより、深層にネストされたコンテンツも含めた、ドキュメントの完全なテキスト抽出を実現している。</p><h4 id="apiレート制限への対応">APIレート制限への対応</h4><p>大量のブロックを再帰的に取得する際、課題となるのが API のリクエスト制限（Rate Limits）である。
Notion API の公式仕様では平均して秒間 3 リクエスト程度が上限とされており、これを超過すると HTTP 429 (Too Many Requests) エラーが返却され、処理が中断してしまう。</p><p>前処理パイプラインでは数千規模のブロックを短時間で処理する必要があるため、単純なループ処理では即座に制限に抵触する。
この課題に対し、本実装では API コール間に一定の待機時間（Sleep）を挿入する制御機構を導入した。
実験的な調整の結果、待機時間を <span class="codespan-content"><code>350ms</code></span>（約 2.85 リクエスト/秒）に設定することで、API 制限を回避しつつ、実用的な時間内での全量データ同期を可能とした。実装の一部を <span class="cross-reference" data-localized-kind="リスト">コード 4.1: レート制限を考慮したデータ取得ロジック（抜粋）</span> に示す。</p><figure><pre><code class="language-typescript">// レート制限対応のための待機時間（約2.85リクエスト/秒に制限）
const RATE_LIMIT_DELAY_MS = 350;

/**
 * 指定時間だけ処理をブロックするユーティリティ関数
 */
async function sleep(ms: number): Promise&lt;void&gt; {
  return new Promise((resolve) =&gt; setTimeout(resolve, ms));
}

// ページごとの処理ループ（概念コード）
for (const result of response.results) {
  if (result.object === &quot;page&quot;) {
    // ページ内容の取得（内部で再帰的にブロックを取得）
    const pageData = await getPageContent(result.id);
    
    if (pageData) {
      pages.push(pageData);
    }
    
    // APIレート制限（HTTP 429）回避のため、リクエスト間に待機時間を挿入
    await sleep(RATE_LIMIT_DELAY_MS);
  }
}</code></pre><figcaption class="caption-bottom" data-localized-kind="リスト">コード 4.1: レート制限を考慮したデータ取得ロジック（抜粋）</figcaption></figure><h4 id="メディアファイルの永続化とリンクの書き換え">メディアファイルの永続化とリンクの書き換え</h4><p>Notion API が返却する画像や添付ファイル（PDF等）の URL は、Amazon S3 の署名付き URL（Signed URL）であり、セキュリティ上の理由から一定時間（通常1時間程度）で有効期限が切れる仕様となっている。
そのため、取得した URL をそのままデータベースに保存し Web アプリケーションで使用すると、時間の経過とともにリンク切れが発生し、画像が表示されなくなるという致命的な問題がある。</p><p>この課題を解決するため、本システムの前処理パイプラインでは、テキストデータの取得と並行してメディアファイルのダウンロード処理を実装した。
具体的には、再帰的なブロック探索の過程で <span class="codespan-content"><code>image</code></span>、<span class="codespan-content"><code>pdf</code></span>、<span class="codespan-content"><code>file</code></span> といったバイナリデータを含むブロックを検知した場合、以下の処理を自動的に実行する。</p><ol><li><strong>データのダウンロード</strong>:
取得した一時 URL にアクセスし、バイナリデータをストリームとして取得する。</li><li><strong>静的ディレクトリへの保存</strong>:
取得したデータを、Next.js が静的アセットとして配信可能な <span class="codespan-content"><code>public/images</code></span> および <span class="codespan-content"><code>public/files</code></span> ディレクトリに保存する。</li><li><strong>参照パスの書き換え</strong>:
ブロックデータ内の URL プロパティを、Notion の一時 URL から、保存したローカルファイルへのパス（例: <span class="codespan-content"><code>/images/pageId_blockId.png</code></span>）へと書き換える。</li></ol><p>この処理により、Notion 側の URL 有効期限に依存することなく、アプリケーション上で永続的にメディアを表示することを可能にした。実装の一部を <span class="cross-reference" data-localized-kind="リスト">コード 4.2: メディアファイルのダウンロードとパス書き換え処理（抜粋）</span> に示す。</p><figure><pre><code class="language-typescript">// メディアタイプごとの保存設定（種別、保存先ディレクトリ、配信パス）
const mediaCases = [
  { type: &quot;image&quot;, key: &quot;imageUrl&quot;, dir: imagesDir, base: &quot;/images&quot; },
  { type: &quot;pdf&quot;,   key: &quot;pdfUrl&quot;,   dir: filesDir,  base: &quot;/files&quot;  },
  { type: &quot;file&quot;,  key: &quot;fileUrl&quot;,  dir: filesDir,  base: &quot;/files&quot;  },
];

for (const c of mediaCases) {
  // ブロックタイプが一致し、かつURLが存在する場合
  if (block.type === c.type &amp;&amp; block[c.key]) {
    const url = block[c.key];
    // 一意なファイル名を生成（ページID_ブロックID）
    const filename = `${pageId}_${block.id}${ext}`;
    const filepath = path.join(c.dir, filename);

    // 1. バイナリデータをローカルのpublicディレクトリへダウンロード
    await downloadImageOrFile(url, filepath);

    // 2. ブロック内のURLをローカルパス（静的アセット）に書き換え
    block[c.key] = `${c.base}/${filename}`;
  }
}</code></pre><figcaption class="caption-bottom" data-localized-kind="リスト">コード 4.2: メディアファイルのダウンロードとパス書き換え処理（抜粋）</figcaption></figure><h3 id="埋め込みベクトルの生成">埋め込みベクトルの生成</h3><p>Notion から取得したテキストデータを検索可能な状態にするためには、自然言語の意味を数値の列（ベクトル）に変換する「埋め込み（Embedding）」処理が必要となる。本節では、採用したモデルの選定根拠および実装詳細について述べる。</p><h4 id="ベクトル化ライブラリの採用">ベクトル化ライブラリの採用</h4><p>本システムでは、Python 環境に依存せず Node.js 環境上で直接機械学習モデルを推論させるため、Hugging Face 社が提供する <span class="codespan-content"><code>transformers.js</code></span> ライブラリを採用した。
これにより、Next.js のサーバーサイド処理およびローカルの前処理スクリプトと、推論エンジンを同一言語（TypeScript/JavaScript）で統合することが可能となり、システムの複雑性を低減している。</p><h4 id="実装上の工夫">実装上の工夫</h4><p>モデルの実装にあたり、精度向上とパフォーマンス最適化のために以下の工夫を取り入れた。</p><h5 id="ベクトル化の粒度と対象データの選別">ベクトル化の粒度と対象データの選別</h5><p>Notion のデータ構造は、ページ（Page）とそれを構成するブロック（Block）の階層構造となっている。本システムでは、ベクトル化の粒度を「ページ単位」ではなく「ブロック単位」に設定した。
これには以下の2つの理由がある。</p><ol><li><p><strong>トークン制限の回避</strong>
採用した <span class="codespan-content"><code>multilingual-e5-small</code></span> モデルの最大入力長は 512 トークンである。ページ全体のテキストを一度にベクトル化しようとすると、長文のページでは末尾のデータが切り捨てられ、検索対象から漏れるリスクがある。ブロック単位であれば、この制限内に収まる確率が極めて高くなる。</p></li><li><p><strong>検索精度の向上</strong>
ページ全体を一つのベクトルに集約すると、多様なトピックが含まれる場合にベクトルの特徴が希釈されてしまう。ブロック単位でベクトル化することで、ユーザーの質問に対してピンポイントな回答を含む段落を正確に抽出することが可能となる。</p></li></ol><p>また、Notion のブロックには画像（image）や区切り線（divider）、空行など、テキスト情報を含まない種別も多数存在する。これらは意味的な検索においてノイズとなるため、本システムでは <span class="codespan-content"><code>paragraph</code></span>、<span class="codespan-content"><code>heading</code></span>（見出し）、<span class="codespan-content"><code>list_item</code></span>（リスト）といったテキストプロパティを保持するブロックタイプのみをフィルタリングし、テキストが存在するブロックのみを個別に抽出してベクトル化処理を行った。</p><h5 id="推論パラメータの設定と計算効率化">推論パラメータの設定と計算効率化</h5><p><span class="codespan-content"><code>transformers.js</code></span> のパイプライン実行時には、以下のオプションを指定することで出力ベクトルの品質と検索効率を最適化している。</p><ol><li><p><strong>平均プーリング (Mean Pooling)</strong>
Transformer モデルの出力は、トークン（単語の一部）ごとのベクトル列である。これを文全体の意味を表す単一の固定長ベクトル（768次元や384次元）に変換するため、<span class="codespan-content"><code>pooling: &quot;mean&quot;</code></span> を指定し、全トークンベクトルの平均値を算出している。これにより、文の長さに関わらず一貫したベクトル表現が得られる。</p></li><li><p><strong>正規化 (Normalization)</strong>
<span class="codespan-content"><code>normalize: true</code></span> を指定することで、生成されたベクトルに対して 正規化を適用し、そのノルム（長さ）が常に 1 となる単位ベクトルに変換している。
この処理は、後段の類似度検索において計算コストを削減するために重要である。コサイン類似度 <formula>S_C(A, B)</formula> は以下の式 <span class="cross-reference" data-location="(4.1)"></span> で定義される。</p><formula data-block="" data-location="(4.1)">S_C(A, B) = \frac{A \cdot B}{\|A\| \|B\|}</formula><p>ここで、ベクトル <formula>A, B</formula> が正規化されている（<formula>\|A\| = 1, \|B\| = 1</formula>）場合、分母は 1 となり、式は単純な内積（ドット積）計算へと簡略化される <span class="cross-reference" data-location="(4.2)"></span>。</p><formula data-block="" data-location="(4.2)">S_C(A, B) = A \cdot B</formula><p>これにより、検索時に都度ノルム計算や除算を行う必要がなくなり、特に大規模なインデックスに対する検索速度の向上に寄与している。</p></li></ol><h5 id="クエリとパッセージの区別">クエリとパッセージの区別</h5><p>採用した E5 モデルは、入力テキストに対して特定のプレフィックス（接頭辞）を付与することで精度が最適化されるよう設計されている。
本システムでは、検索クエリには <span class="codespan-content"><code>&quot;query: &quot;</code></span>、検索対象となるドキュメント（Notionデータ）には <span class="codespan-content"><code>&quot;passage: &quot;</code></span> を自動的に付与してベクトル化を行うロジックを実装した。</p><h4 id="埋め込みモデルの比較選定と性能検証">埋め込みモデルの比較選定と性能検証</h4><p>本システムにおける埋め込みモデルの選定にあたり、多言語対応モデルとして評価の高い <span class="codespan-content"><code>multilingual-e5</code></span> シリーズ（Small, Base, Large）を候補とした。
採用するライブラリ <span class="codespan-content"><code>transformers.js</code></span> は、Web 実行環境向けに最適化（INT8量子化）されたモデルを提供しており、各モデルのファイルサイズは以下の通りである。</p><ul><li><strong>Small</strong>: <span class="codespan-content"><code>intfloat/multilingual-e5-small</code></span> (約 118MB)</li><li><strong>Base</strong>: <span class="codespan-content"><code>intfloat/multilingual-e5-base</code></span> (約 278MB)</li><li><strong>Large</strong>: <span class="codespan-content"><code>intfloat/multilingual-e5-large</code></span> (約 1.1GB)</li></ul><h5 id="実行環境における制約メモリとコールドスタート">実行環境における制約（メモリとコールドスタート）</h5><h5 id="実行環境における制約メモリとコールドスタート">実行環境における制約（メモリとコールドスタート）</h5><p>本システムは AWS Amplify を用いたサーバーレス環境（AWS Lambda 等）での稼働を前提としている。
AWS Amplify Hosting のデフォルト設定では、Next.js (SSR) 実行環境のメモリ割り当て上限は <strong>1024MB</strong> である。</p><p>この環境下では、機械学習モデルの重みだけでなく、Node.js ランタイム、Next.js フレームワークのオーバーヘッド、およびインデックスデータのメモリ展開に必要なリソースをすべて合計した物理メモリ使用量（RSS）が、この上限内に収まる必要がある。
実環境へのデプロイを行い、メモリ使用状況を計測したログデータを解析した結果、以下の事実が判明した。</p><ol><li><p><strong>Smallモデルでの限界</strong>
最も軽量な <span class="codespan-content"><code>multilingual-e5-small</code></span> を採用した場合でも、モデルのロードおよびインデックスの展開処理直後のメモリ使用量は <strong>933.42MB</strong> に達した。これは割り当て上限（1024MB）の <strong>91.2%</strong> を占有しており、余剰リソースはわずか 90MB 程度である。</p></li><li><p><strong>Base/Largeモデルの不可能性</strong>
一つ上のサイズである <span class="codespan-content"><code>multilingual-e5-base</code></span> は、Small モデルと比較してファイルサイズだけで約 160MB 増加する。
単純計算でも $933\text{MB} + 160\text{MB} \approx 1093\text{MB}$ となり、アプリケーション本体のメモリ消費と合算すると確実に上限を超過する。</p></li></ol><p>これにより、メモリ不足（Out of Memory）によるプロセス強制終了を回避し、安定稼働を実現するためには、<strong>Small モデルが唯一の選択肢</strong>であることが定量的に示された。</p><h5 id="検索精度の実証実験">検索精度の実証実験</h5><p>前述の通り、運用環境の物理的制約から <span class="codespan-content"><code>multilingual-e5-small</code></span> が唯一の選択肢となるが、パラメータ数の削減によって検索精度、特に専門的な技術概念の識別能力が損なわれていないかを検証する必要がある。
そこで、キーワードが共通しており、文脈による判別が必要な技術用語を対象とした小規模なベンチマークを実施した。</p><p><strong>1. 実験設定</strong>
以下の2つのトピックについて、類似するが異なる概念を持つドキュメントペアと、それらを区別するための検索クエリを用意した。</p><ul><li><strong>Gitの統合手法</strong>: <span class="codespan-content"><code>Merge</code></span>（履歴を残す）と <span class="codespan-content"><code>Rebase</code></span>（履歴を書き換える）</li><li><strong>ブラウザストレージ</strong>: <span class="codespan-content"><code>LocalStorage</code></span>（永続的）と <span class="codespan-content"><code>SessionStorage</code></span>（一時的）</li></ul><p>具体的には、本システムが<strong>Notionのブロック（Block）単位</strong>でデータを管理・検索していることを踏まえ、ページ全体のような長文ではなく、単一の段落やリストアイテムに相当する以下の短いテキストデータをインデックスに登録した。</p><ol><li><strong>Git Mergeに関するブロックデータ ($doc_1$)</strong></li></ol><ul><li>「Git Mergeは、分岐したブランチの履歴をそのまま残し、新しいコミットを作って統合する方法です。過去の経緯が詳細に残ります。」</li></ul><ol start="2"><li><strong>Git Rebaseに関するブロックデータ ($doc_2$)</strong></li></ol><ul><li>「Git Rebaseは、分岐元の履歴を書き換え、ブランチが一本になるように統合する方法です。履歴は綺麗になりますが元の経緯は消えます。」</li></ul><ol start="3"><li><strong>LocalStorageに関するブロックデータ ($doc_3$)</strong></li></ol><ul><li>「LocalStorageは、ブラウザに保存されるデータで、サーバーへは自動送信されません。明示的に消さない限り、ウィンドウを閉じても永続的に残ります。」</li></ul><ol start="4"><li><strong>SessionStorageに関するブロックデータ ($doc_4$)</strong></li></ol><ul><li>「SessionStorageは、ブラウザに保存されるデータですが、タブやウィンドウを閉じると即座に削除されます。サーバーへは送信されません。」</li></ul><p>これらのドキュメントに対し、**ユーザーの検索意図を反映したクエリ（詳細は表 4.5 を参照）**を投入し、意図通りのドキュメントを識別できるか検証を行った。</p><p><strong>2. 評価方法</strong>
採用候補である <span class="codespan-content"><code>multilingual-e5-small</code></span>（量子化版）を用いてドキュメントとクエリをベクトル化し、コサイン類似度を算出する。各クエリに対して、正解となるドキュメントが最も高いスコア（Top1）を獲得できるかを確認する。</p><p><strong>3. 実験結果</strong>
実験の結果を表 <span class="cross-reference" data-location="4.1" data-localized-kind="テーブル"></span> に示す。</p><table id="table-4.1"><thead><tr><th align="left">検索クエリ（意図）</th><th align="left">Top1 (正解)</th><th align="left">スコア</th><th align="left">Top2 (不正解)</th><th align="left">スコア</th></tr></thead><tbody><tr><td align="left"><strong>履歴を書き換えて一直線にする</strong>&lt;br&gt;(Rebase狙い)</td><td align="left"><strong>Git Rebase</strong></td><td align="left"><strong>0.8602</strong></td><td align="left">Git Merge</td><td align="left">0.8410</td></tr><tr><td align="left"><strong>履歴構造を維持したままマージ</strong>&lt;br&gt;(Merge狙い)</td><td align="left"><strong>Git Merge</strong></td><td align="left"><strong>0.8588</strong></td><td align="left">Git Rebase</td><td align="left">0.8534</td></tr><tr><td align="left"><strong>ブラウザを閉じても残り続ける</strong>&lt;br&gt;(LocalStorage狙い)</td><td align="left"><strong>LocalStorage</strong></td><td align="left"><strong>0.9017</strong></td><td align="left">SessionStorage</td><td align="left">0.8951</td></tr><tr><td align="left"><strong>タブを閉じた瞬間に破棄される</strong>&lt;br&gt;(SessionStorage狙い)</td><td align="left"><strong>SessionStorage</strong></td><td align="left"><strong>0.8873</strong></td><td align="left">LocalStorage</td><td align="left">0.8789</td></tr></tbody><caption class="caption-bottom" data-location="4.1" data-localized-kind="テーブル">表 4.5: 技術用語における文脈識別精度の検証結果</caption></table><p><strong>4. 考察</strong>
実験の結果、全てのクエリにおいて正解ドキュメントが1位にランク付けされた。
Gitの例では、「履歴」「統合」といった共通キーワードがある中で「書き換える」対「維持する」という動詞的なニュアンスを識別できており、ストレージの例でも「保存」という共通語に惑わされず「永続性」と「一時性」を正確に区別できている。
この結果より、<span class="codespan-content"><code>multilingual-e5-small</code></span> は軽量モデルでありながら、本システムが対象とするNotionブロックのテキストデータの検索において、実用上十分なセマンティック検索能力を有していることが確認できた。</p><h5 id="選定結論">選定結論</h5><p>以上の検証結果より、本システムの運用環境（サーバーレス環境）におけるリソース制約をクリアし、かつ十分な検索精度を提供可能な唯一の選択肢として、<strong><span class="codespan-content"><code>multilingual-e5-small</code></span>（量子化版）</strong> を採用することとした。</p><h3 id="インデックスの構築戦略と最適化">インデックスの構築戦略と最適化</h3><p>生成された埋め込みベクトルを高速に検索するため、本システムでは HNSW (Hierarchical Navigable Small World) アルゴリズムを採用している。
HNSW は、スモールワールドグラフの特性を利用した階層的なグラフ構造を持つ近似最近傍探索（ANN）手法であり、高次元データに対しても対数オーダーの計算量で検索が可能である [???]。</p><p>本節では、検索精度と速度のバランスを最適化するためのパラメータ選定、およびサーバーレス環境におけるリソース制約を克服するための構築・デプロイ戦略について述べる。</p><h4 id="パラメータの選定実験">パラメータの選定実験</h4><p>HNSW の性能は、主に以下の3つのハイパーパラメータに依存する。</p><ol><li>** $M$ **: 各ノードがグラフ内で保持する接続（エッジ）の最大数。値を大きくすると検索精度（Recall）は向上するが、メモリ消費量と構築時間が増加する [???]。</li><li><strong><formula>efConstruction</formula></strong>: インデックス構築時の探索候補リストのサイズ。品質と構築時間のトレードオフに関与する。</li><li>** <formula>efSearch</formula> **: 検索時の探索候補リストのサイズ。検索速度と精度のトレードオフを直接的に制御する。</li></ol><p>本システムでは、検索精度（Recall@10）が 0.95 以上であることを要件とし、これを満たす最速のパラメータ設定を決定するための予備実験を行った。</p><p><strong>1. 実験条件</strong></p><ul><li><strong>データセット</strong>: <span class="codespan-content"><code>multilingual-e5-small</code></span> の出力次元（384次元）に準拠した乱数ベクトル 10,000件。<ul><li>件数の根拠: 個人のナレッジベースにおける標準的な規模（約200ページ×50ブロック）を想定。</li></ul></li><li><strong>評価指標</strong>: Recall（再現率）および平均検索時間（ms）。</li></ul><p><strong>2. 実験結果と考察</strong>
実験結果の一部を表 [???] に示す。</p><table id="table-4.2"><thead><tr><th align="left">M</th><th align="left">efConstruction</th><th align="left">efSearch</th><th align="left">Recall</th><th align="left">Avg Time (ms)</th><th align="left">備考</th></tr></thead><tbody><tr><td align="left">4</td><td align="left">100</td><td align="left">500</td><td align="left">0.648</td><td align="left">0.662</td><td align="left">精度不足</td></tr><tr><td align="left">16</td><td align="left">100</td><td align="left">380</td><td align="left">0.951</td><td align="left">1.512</td><td align="left">目標達成</td></tr><tr><td align="left"><strong>24</strong></td><td align="left"><strong>100</strong></td><td align="left"><strong>210</strong></td><td align="left"><strong>0.953</strong></td><td align="left"><strong>1.437</strong></td><td align="left"><strong>最適値（最速）</strong></td></tr><tr><td align="left">48</td><td align="left">100</td><td align="left">250</td><td align="left">0.990</td><td align="left">1.961</td><td align="left">過剰スペック</td></tr></tbody><caption class="caption-bottom" data-location="4.2" data-localized-kind="テーブル"></caption></table><p>&rdquo; 表 4.3: HNSWパラメータのベンチマーク結果（抜粋）&rdquo; {#tbl:hnsw_benchmark}</p><p>実験の結果、Recall $\ge 0.95$ を達成する設定の中で、<strong><formula>M=24, efSearch=210</formula></strong> の組み合わせが最も高速（1.437ms）であったため、本システムではこの設定を採用した。</p><h4 id="サーバーレス環境への適応インデックスハイドレーション">サーバーレス環境への適応（インデックス・ハイドレーション）</h4><p>最適なパラメータが決定したが、HNSW インデックスの構築（<span class="codespan-content"><code>initIndex</code></span> および <span class="codespan-content"><code>addPoint</code></span>）は、データ量 $N$ に対して $O(N \log N)$ の計算量を要する処理である。
AWS Lambda のようなサーバーレス環境において、リクエストやコンテナ起動のたびにこの構築処理を実行することは、コールドスタート時間の増大と CPU リソースの浪費を招くため現実的ではない。
また、実行時のファイルシステムが一時的（Ephemeral）であるため、静的ファイルへのパス解決が不安定になるという課題もある。</p><p>これらの問題を解決するため、本システムではインデックスを事前に構築し、アプリケーションコードの一部としてバンドルする「インデックス・ハイドレーション（Index Hydration）」機構を実装した。</p><ol><li><p><strong>ビルド時（Build Time）</strong>:
前処理パイプラインにおいて、決定したパラメータ（<formula>M=24</formula> 等）を用いてインデックスを構築し、そのバイナリデータを <strong>Base64 文字列</strong> に変換して TypeScript のソースコードに直接埋め込む。</p></li><li><p><strong>実行時（Runtime）</strong>:
アプリケーション起動時、埋め込まれたデータをデコードしてメモリ上に展開することで、計算コストの高い構築プロセスを完全にスキップする。</p></li></ol><p>この戦略により、高度なベクトル検索機能と、サーバーレス環境に求められる高速な起動・応答性能の両立を実現している。</p><h3 id="amplifyへの更新データのデプロイ">Amplifyへの更新データのデプロイ</h3><p>本システムでは、前処理パイプラインによって生成された検索インデックスおよびローカル保存されたメディアファイルを、本番環境へ反映させるため、GitHub と AWS Amplify を連携させた CI/CD（継続的デリバリー）パイプラインを構築した。</p><h3 id="github-連携による自動デプロイ">GitHub 連携による自動デプロイ</h3><p>AWS Amplify は、指定された GitHub リポジトリの特定のブランチ（本システムでは <span class="codespan-content"><code>main</code></span> ブランチ）を監視する Webhook 機能を備えている。
前処理パイプラインの実行が完了し、更新されたインデックスファイルおよび <span class="codespan-content"><code>public/</code></span> ディレクトリ内のメディアファイルが Git 経由でリモートリポジトリへプッシュされると、AWS Amplify はこれを即座に検知し、ビルドおよびデプロイプロセスを自動的に開始する。</p><p>この仕組みにより、開発者が AWS コンソールを直接操作することなく、ローカルでのデータ更新作業から数分以内に、最新の検索データが本番環境へ反映されるワークフローを実現した。</p><h3 id="更新フローのまとめ">更新フローのまとめ</h3><p>本システムにおけるデータ更新の全体フローは以下の通りである。</p><ol><li><strong>ローカル実行</strong>: 開発者がローカル環境で前処理スクリプトを実行し、最新の Notion データを取得・ベクトル化し、インデックスを構築する。</li><li><strong>コミットとプッシュ</strong>: 生成されたインデックスおよび保存したメディアファイルを Git でコミットし、GitHub へプッシュする。</li><li><strong>自動デプロイ</strong>: AWS Amplify がリポジトリの変更を検知し、Next.js アプリケーションのビルドと公開を自動実行する。</li></ol><p>このように、更新データをリポジトリへプッシュするだけで、Webサイト上の検索データが常に最新の状態に同期される仕組みを構築した。</p><h2 id="webアプリケーション実行環境" data-location="4.2">Webアプリケーション実行環境</h2><figure id="figure-4.2"><img src="media/webExecutionEnv@958386130.png" alt="Quarkdown" title="Webアプリケーション実行環境の処理フロー" style="width: 90.0%;" /><figcaption class="caption-bottom" data-location="4.2" data-localized-kind="図">Webアプリケーション実行環境の処理フロー</figcaption></figure><p><span class="cross-reference" data-location="4.2" data-localized-kind="図"></span> に、Webアプリケーション実行環境における詳細な処理フローを示す。
本システムは、Next.js の App Router を基盤とし、静的コンテンツの配信と動的な検索処理を明確に分離したアーキテクチャを採用している。
以下に、ユーザーのアクセスから検索結果が表示されるまでの主要なプロセスについて述べる。</p><h3 id="ssgでの初期画面表示戦略">SSGでの初期画面表示戦略</h3><h3 id="notionブロックの各コンポーネントのレンダリング">Notionブロックの各コンポーネントのレンダリング</h3><h3 id="クエリの検索処理">クエリの検索処理</h3><h4 id="キャッシュ復元">キャッシュ復元</h4><h4 id="サーバーアクションによるクライアントに負荷をかけないエンべディング検索処理">サーバーアクションによるクライアントに負荷をかけないエンべディング、検索処理</h4><h4 id="検索時のシングルトンパターンによるメモリ管理">検索時のシングルトンパターンによるメモリ管理</h4><p><span class="codespan-content"><code>transformers.js</code></span> のパイプライン初期化はコストの高い処理である。
特に AWS Lambda 環境において、リクエストごとにモデルを再ロードすることはメモリ枯渇（OOM）やレスポンス遅延の原因となる。
これ防ぐため、<span class="codespan-content"><code>LocalEmbeddings</code></span> クラスをシングルトンパターンで実装し、インスタンスおよびモデルをメモリ上にキャッシュすることで、2回目以降の推論を高速化した。
実装したクラスの概要を <span class="cross-reference" data-localized-kind="リスト">コード 4.2: 埋め込みベクトル生成クラス（LocalEmbeddings）</span> に示す。</p><figure><pre><code class="language-typescript">export class LocalEmbeddings {
  private static instance: LocalEmbeddings | null = null;
  private modelName = &quot;Xenova/multilingual-e5-small&quot;; // 採用モデル

  // シングルトンインスタンスの取得
  static getInstance(): LocalEmbeddings {
    if (!LocalEmbeddings.instance) {
      LocalEmbeddings.instance = new LocalEmbeddings();
    }
    return LocalEmbeddings.instance;
  }

  // ベクトル化処理
  async embedQuery(text: string, isQuery: boolean = true): Promise&lt;number[]&gt; {
    if (!this.model) await this.initialize();

    // E5モデル特有のプレフィックス付与
    // query: 検索文用, passage: ドキュメント用
    let processedText = text.length &gt; 512 ? text.substring(0, 512) : text;
    if (this.modelName.includes(&quot;e5&quot;)) {
      processedText = isQuery
        ? `query: ${processedText}`
        : `passage: ${processedText}`;
    }

    // 推論実行（平均プーリング・正規化）
    const result = await this.model(processedText, {
      pooling: &quot;mean&quot;,
      normalize: true,
    });

    return Array.from(result.data as Float32Array);
  }
}</code></pre><figcaption class="caption-bottom" data-localized-kind="リスト">コード 4.2: 埋め込みベクトル生成クラス（LocalEmbeddings）</figcaption></figure><h3 id="結果表示uiの工夫">結果表示UIの工夫</h3><div class="page-break" data-hidden=""></div><h1 id="動作検証" data-location="5">動作検証</h1><div class="page-break" data-hidden=""></div><h1 id="終わりに" data-location="6">終わりに</h1>
</body>
</html>